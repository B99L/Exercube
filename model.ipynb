{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HWerJCcL5O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4760be84-fec7-459e-fb77-a60b55f97d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "drive.mount('/content/drive')\n",
        "# Pfad zur CSV-Datei in Google Drive\n",
        "file_path = '/content/drive/My Drive/combined_data.csv'\n",
        "\n",
        "# Einlesen der CSV-Datei\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtClReg5L2hP"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "Alle Probanden  (Sqaut und Jump Bewegungen)\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Daten laden\n",
        "#data = pd.read_csv(r'C:\\Users\\badrl\\OneDrive - ZHAW\\Desktop\\combined_data.csv')\n",
        "data = data[data['Exercise'].isin(['Squat', 'Jump'])]\n",
        "\n",
        "# Entfernen von 'Timestamp' und 'Subject'\n",
        "data = data.drop(columns=['Timestamp', 'Subject'])\n",
        "\n",
        "# One-Hot-Encoding für 'Exercise'\n",
        "data = pd.get_dummies(data, columns=['Exercise'])\n",
        "# Normalisierung der numerischen Daten\n",
        "# Erstellen eines Scalers\n",
        "scaler = StandardScaler()\n",
        "# Wählen Sie die numerischen Spalten aus\n",
        "numeric_columns = data.select_dtypes(include=['float64', 'int64','bool']).columns\n",
        "# Anwenden des Scalers auf die numerischen Spalten\n",
        "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
        "\n",
        "# Umwandlung der Daten in Sequenzen von je 240 Zeilen\n",
        "sequences = [data.iloc[i:i+240] for i in range(0, len(data), 240)]\n",
        "\n",
        "# Stellen Sie sicher, dass die letzte Sequenz vollständig ist\n",
        "sequences = [seq for seq in sequences if len(seq) == 240]\n",
        "\n",
        "# Umwandlung jeder Sequenz in einen Tensor\n",
        "tensor_sequences = [torch.tensor(sequence.values).float() for sequence in sequences]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transformer-Autoencoder\n",
        "'''\n",
        "#Libraries\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly\n",
        "import plotly.graph_objs as go\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.interpolate import make_interp_spline\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.spatial.distance import cdist\n",
        "import seaborn as sns\n",
        "import onnx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500, device=torch.device('cpu')):\n",
        "        super().__init__()\n",
        "        # Erstellen einer Positionskodierungsmatrix\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).to(device)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1).to(device)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Addiert die Positionskodierung zur Eingabe\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class TransformerAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, num_layers=8):\n",
        "        super(TransformerAutoencoder, self).__init__()\n",
        "        # Initialisierung der Klassenvariablen und der Positionskodierung\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.positional_encoder = PositionalEncoding(d_model=input_dim)\n",
        "\n",
        "        # Lineare Reduktion der Dimensionen mit Normalisierung\n",
        "        self.linear_reduction = nn.Linear(input_dim, latent_dim)\n",
        "        self.linear_reduction_norm = nn.LayerNorm(latent_dim)\n",
        "\n",
        "        # Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=2, norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=latent_dim, nhead=2, norm_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Zusätzliche lineare Schichten und Normalisierung\n",
        "        self.linear1 = nn.Linear(latent_dim, latent_dim * 2)\n",
        "        self.linear1_norm = nn.LayerNorm(latent_dim * 2)  # Layer-Normalisierung hinzugefügt\n",
        "        self.linear2 = nn.Linear(latent_dim * 2, latent_dim)\n",
        "        self.linear2_norm = nn.LayerNorm(latent_dim)  # Layer-Normalisierung hinzugefügt\n",
        "\n",
        "        # Rekonstruktions-Output\n",
        "        self.decoder_output = nn.Linear(latent_dim, input_dim)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        # Kodierungsprozess: Positionskodierung, lineare Reduktion und Durchlauf durch den Transformer Encoder\n",
        "        x = self.positional_encoder(x)\n",
        "        x = self.linear_reduction(x)\n",
        "        x = self.linear_reduction_norm(x)\n",
        "        encoded_x = self.transformer_encoder(x)\n",
        "        return encoded_x\n",
        "\n",
        "    def decode(self, encoded_x):\n",
        "      # Dekodierungsprozess: Durchlauf durch den Transformer Decoder und lineare Transformation\n",
        "      decoded_x = self.transformer_decoder(encoded_x, torch.zeros_like(encoded_x))\n",
        "      decoded_x = F.relu(self.linear1(decoded_x))\n",
        "      decoded_x = self.linear2(decoded_x)\n",
        "      decoded_x = self.decoder_output(decoded_x)\n",
        "      return decoded_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Gesamter Prozess: Kodieren und Dekodieren\n",
        "        encoded_x = self.encode(x)\n",
        "        recon_x = self.decode(encoded_x)\n",
        "        return recon_x\n",
        "\n",
        "# Verlustfunktion\n",
        "def loss_function(recon_x, x, delta=1):\n",
        "    recon_loss = F.smooth_l1_loss(recon_x, x, reduction='mean', beta=delta)\n",
        "    return recon_loss\n",
        "\n",
        "#Model Trainieren\n",
        "def pretrain_autoencoder(model, train_loader, val_loader, optimizer, epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            reconstructed = model(data)\n",
        "            loss = loss_function(reconstructed, data)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "        train_losses.append(total_train_loss)\n",
        "\n",
        "        # Validierung\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                data = data.to(device)\n",
        "                reconstructed = model(data)\n",
        "                loss = loss_function(reconstructed, data)\n",
        "                total_val_loss += loss.item()\n",
        "        val_losses.append(total_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Train Loss: {total_train_loss}, Val Loss: {total_val_loss}\")\n",
        "\n",
        "\n",
        "        if epoch == epochs - 1:\n",
        "            # Extrahieren und Rekonstruieren einer Sequenz aus dem Trainingsset\n",
        "            train_example = next(iter(train_loader))\n",
        "            train_seq = train_example.to(device)\n",
        "            recon_train_seq = model(train_seq)\n",
        "\n",
        "            # Extrahieren und Rekonstruieren einer Sequenz aus dem Validierungsset\n",
        "            val_example = next(iter(val_loader))\n",
        "            val_seq = val_example.to(device)\n",
        "            recon_val_seq = model(val_seq)\n",
        "\n",
        "            # Plotten des Durchschnitts der Features von Trainings- und Validierungsset\n",
        "            plot_average_feature_comparison(train_seq[0].detach().cpu().numpy(), recon_train_seq[0].detach().cpu().numpy(), 'Vergleich von originalen und rekonstruirten Trainigsdaten')\n",
        "            plot_average_feature_comparison(val_seq[0].detach().cpu().numpy(), recon_val_seq[0].detach().cpu().numpy(), 'Vergleich von originalen und rekonstruirten Validierungsdaten')\n",
        "\n",
        "            recon_error_train = loss_function(recon_train_seq, train_seq).item()\n",
        "            recon_error_val = loss_function(recon_val_seq, val_seq).item()\n",
        "\n",
        "    plot_train_val_loss(train_losses, val_losses)\n",
        "\n",
        "\n",
        "# Losses plotten\n",
        "def plot_train_val_loss(train_losses, val_losses):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Trainingsverlust')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validierungsverlust')\n",
        "    plt.title('Trainings- und Validierungsverlust über die Epochen')\n",
        "    plt.xlabel('Epochen')\n",
        "    plt.ylabel('Verlust')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    wandb.log({\"Trainings- und Validierungsverlust über die Epochen\": wandb.Image(plt.gcf())})\n",
        "    plt.show()\n",
        "\n",
        "#Originale und Rekonstruirten Daten Plotten\n",
        "def plot_average_feature_comparison(original_data, reconstructed_data, title):\n",
        "    \"\"\"Funktion zum Plotten des Durchschnitts über alle Features.\"\"\"\n",
        "    avg_original = np.mean(original_data, axis=1)\n",
        "    avg_reconstructed = np.mean(reconstructed_data, axis=1)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(avg_original, label='Originale Daten')\n",
        "    plt.plot(avg_reconstructed, label=' Rekonstruirte Daten', alpha=0.7)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Sequenzlänge')\n",
        "    plt.ylabel('Durchschnittlicher Merkmalswert')\n",
        "    plt.legend()\n",
        "    wandb.log({\"Original Data-Reconstructed data\": wandb.Image(plt.gcf())})\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#Visualisierung der Datenverteilung von Normale und Anomale\n",
        "def visualize_data_distribution(model, train_loader, test_loader, device=torch.device('cpu'), percentile=95, num_features_to_plot=5):\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Berechnung des Schwellenwerts für den Rekonstruktionsfehler\n",
        "    train_errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            reconstructed = model(batch)\n",
        "            batch_error = torch.mean((batch - reconstructed) ** 2, dim=[1, 2])\n",
        "            train_errors.extend(batch_error.cpu().numpy())\n",
        "    threshold = np.percentile(train_errors, percentile)\n",
        "\n",
        "    # Klassifizierung der Testdaten als normal oder anomal\n",
        "    normal_data = []\n",
        "    anomalous_data = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            reconstructed = model(batch)\n",
        "            batch_error = torch.mean((batch - reconstructed) ** 2, dim=[1, 2])\n",
        "\n",
        "            for i in range(batch.size(0)):\n",
        "                if batch_error[i].item() > threshold:\n",
        "                    anomalous_data.append(batch[i].cpu().numpy())\n",
        "                else:\n",
        "                    normal_data.append(batch[i].cpu().numpy())\n",
        "\n",
        "    # Umwandeln in NumPy-Arrays\n",
        "    normal_data = np.array(normal_data).reshape(-1, batch.shape[2])  # Flattening der Sequenzen\n",
        "    anomalous_data = np.array(anomalous_data).reshape(-1, batch.shape[2])\n",
        "\n",
        "    # Visualisierung der Datenverteilung für ausgewählte Features\n",
        "    for i in range(min(num_features_to_plot, batch.shape[2])):\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.histplot(normal_data[:, i], color=\"blue\", kde=True, stat=\"density\", linewidth=0, label=\"Normal\")\n",
        "        sns.histplot(anomalous_data[:, i], color=\"red\", kde=True, stat=\"density\", linewidth=0, label=\"Anomal\")\n",
        "        plt.title(f'Verteilung des Features {i}')\n",
        "        plt.legend()\n",
        "        wandb.log({\"Verteilung des Features\": wandb.Image(plt.gcf())})\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def anomalies_identification_with_ReconstructionError(model, train_loader, test_loader, percentile=95, device=torch.device('cpu')):\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # Berechnung des Schwellenwerts basierend auf den Trainingsdaten\n",
        "    train_errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            reconstructed = model(batch)\n",
        "            batch_error = F.smooth_l1_loss(reconstructed, batch, reduction='none')\n",
        "            batch_error = batch_error.view(batch.size(0), -1).mean(dim=1)\n",
        "            train_errors.extend(batch_error.cpu().numpy())\n",
        "    threshold = np.percentile(train_errors, percentile)\n",
        "\n",
        "    # Berechnung der Rekonstruktionsfehler für die Testdaten\n",
        "    test_errors = []\n",
        "    anomalies = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = batch.to(device)\n",
        "            reconstructed = model(batch)\n",
        "            for i in range(batch.size(0)):\n",
        "                error = F.smooth_l1_loss(reconstructed[i], batch[i], reduction='mean').item()\n",
        "                test_errors.append(error)\n",
        "                anomalies.append(error > threshold)\n",
        "\n",
        "    # Trennung der Fehler in normal und anomal\n",
        "    normal_errors = np.array(test_errors)[~np.array(anomalies)]\n",
        "    anomalous_errors = np.array(test_errors)[np.array(anomalies)]\n",
        "\n",
        "    # Plotten der Fehler\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(range(len(test_errors)), test_errors, c=anomalies, cmap='coolwarm', label='Rekonstruktionsfehler')\n",
        "    plt.axhline(y=threshold, color='r', linestyle='--', label='Schwellenwert')\n",
        "    plt.xlabel('Sequenzindex')\n",
        "    plt.ylabel('Rekonstruktionsfehler')\n",
        "    plt.title('Rekonstruktionsfehler pro Sequenz (Bewegung)')\n",
        "    plt.legend()\n",
        "\n",
        "    # Erstellen von DataFrames für normale und anomale Daten\n",
        "    df_normal = pd.DataFrame({'Fehler': normal_errors, 'Typ': 'Normal'})\n",
        "    df_anomal = pd.DataFrame({'Fehler': anomalous_errors, 'Typ': 'Anomal'})\n",
        "\n",
        "    # Zusammenführen der DataFrames\n",
        "    df_combined = pd.concat([df_normal, df_anomal])\n",
        "\n",
        "    # Statistische Vergleiche\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x='Typ', y='Fehler', data=df_combined)\n",
        "    plt.title('Vergleich der Fehlerstatistiken')\n",
        "    plt.ylabel('Rekonstruktionsfehler')\n",
        "    wandb.log({\"Vergleich der Fehlerstatistiken\": wandb.Image(plt.gcf())})\n",
        "    plt.show()\n",
        "\n",
        "    # Identifizieren von Anomalien\n",
        "    print(f\"Identifizierte Anomalien in den Sequenzen: {np.where(anomalies)[0]}\")\n",
        "    print(\"Anzahl Anomalien: \", sum(anomalies))\n",
        "    visualize_data_distribution(model, train_loader, test_loader, device=device, percentile=percentile)\n",
        "\n",
        "    wandb.log({\"Anomalien Identifikation mit Rekonstruktionsfehler\": wandb.Image(plt.gcf())})\n",
        "\n",
        "\n",
        "#Berechnung den Durchschnittlichen Rekonstruktionsfehler\n",
        "def calculate_average_reconstruction_error(model, data_loader, device=torch.device('cpu')):\n",
        "    model.to(device).eval()\n",
        "    total_error = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            reconstructed = model(data)\n",
        "            error = F.smooth_l1_loss(reconstructed, data, reduction='mean').item()  # Summe der Fehler\n",
        "            total_error += error\n",
        "    return total_error\n",
        "\n",
        "#Extraktion latenter Repräsentationen\n",
        "def extract_latent_representations(dataloader, model):\n",
        "    model.eval()\n",
        "    latent_representations = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            data = batch.to(device)\n",
        "            latent = model.encode(data)  # Latente Repräsentation\n",
        "            if latent.dim() > 2:\n",
        "                latent = latent.mean(dim=1)\n",
        "            latent_representations.append(latent.cpu().numpy())\n",
        "    return np.concatenate(latent_representations)\n",
        "\n",
        "#K-Means Clustering auf latenten Repräsentationen\n",
        "def anomalies_identification_with_Kmeans(train_loader, test_loader, autoencoder):\n",
        "    # Extrahieren der latenten Repräsentationen\n",
        "    latent_representations_train = extract_latent_representations(train_loader, autoencoder)\n",
        "    latent_representations_test = extract_latent_representations(test_loader, autoencoder)\n",
        "\n",
        "    # K-Means Clustering auf den Trainingsdaten\n",
        "    kmeans = KMeans(n_clusters=2, n_init=2, random_state=0)\n",
        "    kmeans.fit(latent_representations_train)\n",
        "\n",
        "\n",
        "    # Konvertieren der Clusterzentren in einen PyTorch-Tensor\n",
        "    cluster_centers_tensor = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32)\n",
        "\n",
        "    # Speichern des Tensors\n",
        "    torch.save(cluster_centers_tensor, 'cluster_centers.pt')\n",
        "\n",
        "    # PCA für Reduzierung auf 2 Dimensionen\n",
        "    pca = PCA(n_components=2)\n",
        "    latent_pca_train = pca.fit_transform(latent_representations_train)\n",
        "    latent_pca_test = pca.transform(latent_representations_test)\n",
        "    cluster_centers_pca = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "    # Zuordnung und Distanzberechnung für Testdaten\n",
        "    test_cluster_labels = kmeans.predict(latent_representations_test)\n",
        "    dists_to_centers = np.min(cdist(latent_representations_test, kmeans.cluster_centers_), axis=1)\n",
        "\n",
        "    # Schwellenwert für Anomalien\n",
        "    threshold = np.percentile(dists_to_centers, 95)\n",
        "    is_anomaly = dists_to_centers > threshold\n",
        "    # Identifizieren der Indizes von Anomalien\n",
        "    anomalous_indices = [i for i, flag in enumerate(is_anomaly) if flag]\n",
        "\n",
        "    # Anomale Sequenzen ausgeben\n",
        "    print(\"Indizes der Anomalien in den Testdaten:\", anomalous_indices)\n",
        "    num_anomalies = np.sum(is_anomaly)\n",
        "\n",
        "    # Ergebnisse ausgeben\n",
        "    print(f\"Anzahl der Anomalien in den Testdaten: {num_anomalies}\")\n",
        "\n",
        "    # Plot für Trainingsdaten\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(x=latent_pca_train[:, 0], y=latent_pca_train[:, 1], hue=kmeans.labels_, palette=\"deep\")\n",
        "    plt.scatter(cluster_centers_pca[:, 0], cluster_centers_pca[:, 1], s=100, c='black', marker='*', label='Cluster Centers')\n",
        "    plt.title(\"K-Means Clustering auf latenten Repräsentationen (Trainingsdaten)\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.legend()\n",
        "    wandb.log({\"K-Means Clustering auf latenten Repräsentationen (Trainingsdaten)\": wandb.Image(plt.gcf())})\n",
        "    plt.show()\n",
        "\n",
        "    # Plot für Testdaten\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(x=latent_pca_test[~is_anomaly, 0], y=latent_pca_test[~is_anomaly, 1], hue=test_cluster_labels[~is_anomaly], palette=\"deep\", legend='full')\n",
        "    sns.scatterplot(x=latent_pca_test[is_anomaly, 0], y=latent_pca_test[is_anomaly, 1], c='green', label='Anomalies', marker='X')\n",
        "    plt.scatter(cluster_centers_pca[:, 0], cluster_centers_pca[:, 1], s=100, c='black', marker='*', label='Cluster Centers')\n",
        "    plt.title(\"K-Means Clustering auf latenten Repräsentationen (Testdaten mit Anomalien)\")\n",
        "    plt.xlabel(\"Principal Component 1\")\n",
        "    plt.ylabel(\"Principal Component 2\")\n",
        "    plt.legend()\n",
        "    wandb.log({\"K-Means Clustering auf latenten Repräsentationen (Testdaten mit Anomalien)\": wandb.Image(plt.gcf())})\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Daten in Trainings-, Validierungs- und Testsets aufteilen\n",
        "train_size = int(0.6 * len(tensor_sequences))\n",
        "val_size = int(0.3 * len(tensor_sequences))\n",
        "test_size = len(tensor_sequences) - train_size - val_size\n",
        "train_set, val_set, test_set = random_split(tensor_sequences, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,drop_last=True)\n",
        "val_loader = DataLoader(val_set, batch_size=64, shuffle=True,drop_last=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "print(\"tensor_sequeces\",len(tensor_sequences ))\n",
        "print(\"train_size\",train_size )\n",
        "print(\"val_size\", val_size)\n",
        "print(\"test_size\", test_size)\n"
      ],
      "metadata": {
        "id": "sR-8znuRLhJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "input_dim = 86  # Anzahl der Features\n",
        "latent_dim = 46# Latente Dimension\n",
        "learning_rate = 0.001\n",
        "epochs = 15\n",
        "\n",
        "#Trainingsumgebung und Modellinitialisierung\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wandb.init(project='ae-model')\n",
        "autoencoder = TransformerAutoencoder(input_dim, latent_dim).to(device)\n",
        "wandb.watch(autoencoder)\n",
        "optimizer = torch.optim.Adam(\n",
        "    autoencoder.parameters(),\n",
        "    lr=learning_rate\n",
        "\n",
        ")\n",
        "#Trainig des Modells\n",
        "pretrain_autoencoder(autoencoder, train_loader, val_loader, optimizer, epochs)\n",
        "\n",
        "#ONNX-Export\n",
        "dummy_input = torch.randn(1, input_dim).to(device)  # Ersetzen Sie 'input_dim' durch die tatsächliche Eingabedimension\n",
        "torch.onnx.export(autoencoder, dummy_input, \"autoencoder.onnx\")\n",
        "\n",
        "# Hochladen des ONNX-Modells zu wandb\n",
        "wandb.save(\"autoencoder.onnx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "WUgQ76slLhL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Berechnen des durchschnittlichen Rekonstruktionsfehlers\n",
        "train_error = calculate_average_reconstruction_error(autoencoder, train_loader, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "val_error = calculate_average_reconstruction_error(autoencoder, val_loader, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "test_error = calculate_average_reconstruction_error(autoencoder, test_loader, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Durchschnittlicher Rekonstruktionsfehler - Training: {train_error}\")\n",
        "print(f\"Durchschnittlicher Rekonstruktionsfehler - Validierung: {val_error}\")\n",
        "print(f\"Durchschnittlicher Rekonstruktionsfehler - Testset: {test_error}\")\n"
      ],
      "metadata": {
        "id": "O5Vg8XpojR__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoader für Testdatensatz\n",
        "anomaly_test_loader = DataLoader(test_set, batch_size=64, shuffle=False, drop_last=True)\n",
        "#Anomalieerkennung mit Rekonstruktionsfehler\n",
        "anomalies_identification_with_ReconstructionError(autoencoder, train_loader, anomaly_test_loader, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EWwarfv6U2dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Anomalieerkennung mit K-means anhand latenten\n",
        "anomalies_identification_with_Kmeans(train_loader, anomaly_test_loader, autoencoder)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "zrEJ-uFwLhQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " DEC Model\n",
        "'''\n",
        "\n",
        "\n",
        "class ClusterAssignmentLayer(nn.Module):\n",
        "    def __init__(self, cluster_centers):\n",
        "        super(ClusterAssignmentLayer, self).__init__()\n",
        "        # Initialisierung der Clusterzentren als trainierbare Parameter\n",
        "        self.cluster_centers = nn.Parameter(torch.Tensor(cluster_centers))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Berechnet die weiche Clusterzuweisung für die Eingabedaten x\n",
        "        # Student's t-Verteilung wird zur Berechnung der Zuweisungswahrscheinlichkeiten verwendet\n",
        "        q = 1.0 / (1.0 + (torch.sum((x.unsqueeze(1) - self.cluster_centers) ** 2, dim=2) / alpha))\n",
        "        q = q ** ((alpha + 1.0) / 2.0)\n",
        "        q = q / torch.sum(q, dim=1, keepdim=True)  # Normalisieren über alle Cluster für jede Probe\n",
        "        return q\n",
        "\n",
        "\n",
        "class DECModel(nn.Module):\n",
        "    def __init__(self, autoencoder, cluster_centers):\n",
        "        super(DECModel, self).__init__()\n",
        "        # Einbettung des Autoencoders und der ClusterAssignmentLayer in das DEC-Modell\n",
        "        self.autoencoder = autoencoder\n",
        "        self.cluster_assignment = ClusterAssignmentLayer(cluster_centers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Kodiert die Eingabe und erhält die latente Darstellung\n",
        "        latent = self.autoencoder.encode(x)\n",
        "         # Dekodiert die latente Darstellung zur Rekonstruktion der Eingabe\n",
        "        reconstructed = self.autoencoder.decode(latent)\n",
        "        # Reduziert die Dimensionalität\n",
        "        if latent.dim() > 2:\n",
        "            latent = latent.mean(dim=1)\n",
        "        # Berechnet die weiche Clusterzuweisung für die latente Darstellung\n",
        "        q = self.cluster_assignment(latent)\n",
        "        return reconstructed, q\n",
        "\n",
        "\n",
        "def target_distribution(q):\n",
        "    # Berechnet die Zielfunktionsverteilung aus den weichen Clusterzuweisungen\n",
        "    weight = q ** 2 / q.sum(0)\n",
        "    # Normalisiert die Verteilung für jede Cluster-Komponente\n",
        "    return (weight.t() / weight.sum(1)).t()\n",
        "\n",
        "def kl_divergence_loss_function(q, p):\n",
        "    # Berechnet die Kullback-Leibler-Divergenz zwischen den Verteilungen q und p\n",
        "    kl_loss = F.kl_div(p.log(), q, reduction='batchmean')\n",
        "    return kl_loss\n",
        "# Verlustfunktion für die Rekonstruktion der Eingabedaten\n",
        "def loss_function(recon_x, x, delta=1):\n",
        "    recon_loss = F.smooth_l1_loss(recon_x, x, reduction='mean', beta=delta)\n",
        "    return recon_loss\n",
        "'''\n",
        "# Clustering-Verlustfunktion\n",
        "def clustering_loss_function(latent, cluster_centers):\n",
        "    dists = torch.cdist(latent, cluster_centers)\n",
        "    min_dists = torch.min(dists, dim=1).values\n",
        "    return torch.mean(min_dists)\n",
        "\n",
        "'''\n",
        "# Haupttrainingsfunktion\n",
        "def train_dec_model(dec_model, train_loader, val_loader, optimizer, epochs, alpha, gamma):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        dec_model.train()\n",
        "        total_train_loss = 0\n",
        "        for data in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            reconstructed, q = dec_model(data)\n",
        "\n",
        "            # Berechnung des Rekonstruktionsverlusts\n",
        "            recon_loss = loss_function(reconstructed, data)\n",
        "\n",
        "            # Aktualisierung der Zielverteilung basierend auf den aktuellen weichen Zuweisungen q\n",
        "            p = target_distribution(q).detach()\n",
        "            # Berechnung des Clustering-Verlusts\n",
        "            clustering_loss = kl_divergence_loss_function(q, p)\n",
        "\n",
        "            # Gesamtverlust mit dem Balancierungsfaktor gamma\n",
        "            loss = recon_loss + gamma * clustering_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        train_losses.append(total_train_loss)\n",
        "\n",
        "        # Validierung\n",
        "        dec_model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in val_loader:\n",
        "                data = data.to(device)\n",
        "                reconstructed, q = dec_model(data)\n",
        "\n",
        "                # Berechnung des Rekonstruktionsverlusts\n",
        "                recon_loss = loss_function(reconstructed, data)\n",
        "\n",
        "                # Keine Notwendigkeit, p während der Validierung zu aktualisieren, da wir nicht zurückpropagieren\n",
        "                clustering_loss = kl_divergence_loss_function(q, target_distribution(q).detach())\n",
        "\n",
        "                # Gesamtverlust für die Validierungsdaten\n",
        "                loss = recon_loss + gamma * clustering_loss\n",
        "                total_val_loss += loss.item()\n",
        "        val_losses.append(total_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch}, Train Loss: {total_train_loss}, Val Loss: {total_val_loss}\")\n",
        "\n",
        "    plot_train_val_loss(train_losses, val_losses)\n",
        "\n",
        "\n",
        "\n",
        "def determine_threshold(distances, percentile=95):\n",
        "  # Bestimmt den Schwellenwert für Anomalien basierend auf dem gegebenen Perzentil der Distanzen\n",
        "    return np.percentile(distances, percentile)\n",
        "\n",
        "def identify_anomalies(distances, threshold):\n",
        "  # Identifiziert Anomalien, indem überprüft wird, ob Distanzen den Schwellenwert überschreiten\n",
        "    return distances > threshold\n",
        "\n",
        "\n",
        "def visualize_with_pca_and_anomalies(model, data_loader, cluster_centers, num_samples=1000, percentile=95):\n",
        "    model.eval()  # Setzt das Modell in den Evaluierungsmodus\n",
        "    latent_space = []  # Speichert latente Darstellungen\n",
        "    cluster_labels = []  # Speichert Clusterzuweisungen\n",
        "    distances_to_center = []  # Speichert Distanzen zu den Clusterzentren\n",
        "\n",
        "    with torch.no_grad():  # Deaktiviert Gradientenberechnungen\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)  # Verschiebt Daten auf das Gerät (z.B. GPU)\n",
        "            latent = model.autoencoder.encode(data)  # Kodiert Daten in den latenten Raum\n",
        "\n",
        "            # Reduziert die Dimensionalität, falls erforderlich\n",
        "            if latent.dim() > 2:\n",
        "                latent = latent.mean(dim=1)\n",
        "\n",
        "            q = model.cluster_assignment(latent)  # Berechnet weiche Clusterzuweisungen\n",
        "            cluster_label = torch.argmax(q, dim=1).cpu().numpy()  # Bestimmt den wahrscheinlichsten Cluster\n",
        "            dists = torch.cdist(latent, cluster_centers).cpu().numpy()  # Berechnet Distanzen zu den Clusterzentren\n",
        "            min_dists = np.min(dists, axis=1)  # Bestimmt minimale Distanz zu Clusterzentren\n",
        "\n",
        "            # Sammelt Informationen für die Analyse\n",
        "            latent_space.append(latent.cpu().numpy())\n",
        "            cluster_labels.append(cluster_label)\n",
        "            distances_to_center.append(min_dists)\n",
        "\n",
        "            # Begrenzt die Anzahl der analysierten Samples\n",
        "            if len(latent_space) >= num_samples:\n",
        "                break\n",
        "\n",
        "    # Konvertiert gesammelte Listen in Arrays und beschränkt sie auf die gewünschte Anzahl von Samples\n",
        "    latent_space = np.concatenate(latent_space)[:num_samples]\n",
        "    cluster_labels = np.concatenate(cluster_labels)[:num_samples]\n",
        "    distances_to_center = np.concatenate(distances_to_center)[:num_samples]\n",
        "\n",
        "    # Bestimmt den Schwellenwert für Anomalien und identifiziert Anomalien\n",
        "    threshold = determine_threshold(distances_to_center, percentile=percentile)\n",
        "    anomalies = identify_anomalies(distances_to_center, threshold)\n",
        "    num_anomalies = np.sum(anomalies)\n",
        "    print(f\"Anzahl der Anomalien: {num_anomalies}\")\n",
        "\n",
        "    # Findet Indizes der Anomalien und druckt sie aus\n",
        "    anomaly_indices = np.where(anomalies)[0]\n",
        "    print(\"Indizes der Anomalien:\", anomaly_indices)\n",
        "\n",
        "    # Berechnet den Silhouetten-Score, falls mehr als ein Cluster vorhanden ist\n",
        "    if len(np.unique(cluster_labels)) > 1:\n",
        "        silhouette_avg = silhouette_score(latent_space, cluster_labels)\n",
        "        print(f\"Silhouetten-Score: {silhouette_avg}\")\n",
        "    else:\n",
        "        print(\"Silhouetten-Score kann nicht berechnet werden, da nur ein Cluster vorhanden ist.\")\n",
        "\n",
        "    # Führt PCA durch, um die Daten auf zwei Dimensionen zu reduzieren\n",
        "    all_points = np.vstack([latent_space, cluster_centers.cpu().numpy()])\n",
        "    pca_results = PCA(n_components=2).fit_transform(all_points)\n",
        "\n",
        "    # Visualisiert die Ergebnisse mit Matplotlib\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Plottet die Datenpunkte, gruppiert nach Clustern\n",
        "    for i in np.unique(cluster_labels):\n",
        "        plt.scatter(pca_results[:-len(cluster_centers)][cluster_labels == i, 0], pca_results[:-len(cluster_centers)][cluster_labels == i, 1], alpha=0.5, label=f'Cluster {i}')\n",
        "    # Plottet die Clusterzentren und Anomalien\n",
        "    plt.scatter(pca_results[-len(cluster_centers):, 0], pca_results[-len(cluster_centers):, 1], c='black', marker='x', label='Clusterzentren', s=100)\n",
        "    plt.scatter(pca_results[:-len(cluster_centers)][anomalies, 0], pca_results[:-len(cluster_centers)][anomalies, 1], c='red', alpha=0.5, label='Anomalien')\n",
        "    # Fügt Titel, Beschriftungen und Legende hinzu\n",
        "    plt.title('Clusterzuordnung und Anomalieerkennung im latenten Raum')\n",
        "    plt.xlabel('PCA Feature 1')\n",
        "    plt.ylabel('PCA Feature 2')\n",
        "    plt.legend()\n",
        "    # Loggt das Bild in wandb\n",
        "    wandb.log({\"Anomalien Identifikation\": wandb.Image(plt.gcf())})\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Daten in Trainings-, Validierungs- und Testsets aufteilen\n",
        "train_size = int(0.6 * len(tensor_sequences))\n",
        "val_size = int(0.3 * len(tensor_sequences))\n",
        "test_size = len(tensor_sequences) - train_size - val_size\n",
        "train_set, val_set, test_set = random_split(tensor_sequences, [train_size, val_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,drop_last=True)\n",
        "val_loader = DataLoader(val_set, batch_size=64, shuffle=True,drop_last=True)\n",
        "#test_loader = DataLoader(test_set, batch_size=64, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "z8vHdbD9MV78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEC-Modell mit initialisierten Clusterzentren\n",
        "cluster_centers = torch.load('cluster_centers.pt').to(device)\n",
        "alpha = 0.5# Freiheitsgrade der Student's t-Verteilung\n",
        "gamma = 1\n",
        "epochen=20\n",
        "\n",
        "wandb.init(project='DEC-model')\n",
        "dec_model = DECModel(autoencoder, cluster_centers).to(device)\n",
        "wandb.watch(dec_model)\n",
        "optimizer = torch.optim.Adam(\n",
        "    dec_model.parameters(),\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "\n",
        "# Trainieren des DEC-Modells\n",
        "train_dec_model(dec_model, train_loader, val_loader, optimizer, epochen, alpha=alpha,gamma=gamma)\n",
        "\n",
        "\n",
        "#ONNX-Export\n",
        "dummy_input = torch.randn(1, input_dim).to(device)  # Ersetzen Sie 'input_dim' durch die tatsächliche Eingabedimension\n",
        "torch.onnx.export(dec_model, dummy_input, \"DEC.onnx\")\n",
        "\n",
        "# Hochladen des ONNX-Modells zu wandb\n",
        "wandb.save(\"DEC.onnx\")\n"
      ],
      "metadata": {
        "id": "biYdVPplkd6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen eines DataLoader für den Testdatensatz zur Anomalieerkennung\n",
        "#anomaly_test_loader = DataLoader(test_set, batch_size=64, shuffle=False, drop_last=True)\n",
        "# Visualisierung der Anomalien mit PCA auf den Daten aus dem Testdatensatz\n",
        "visualize_with_pca_and_anomalies(dec_model, anomaly_test_loader, cluster_centers, percentile=95)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "QalVu9lilOZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cu2Tr93wujQi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}